<!DOCTYPE html>
<html>
  <head>
    <title>Political Information, New Media, and Twitter bots</title>
    <meta charset="utf-8">
    <link href="index_files/remark-css/robot.css" rel="stylesheet" />
    <link href="index_files/remark-css/robot-fonts.css" rel="stylesheet" />
    <link href="index_files/font-awesome/css/font-awesome.min.css" rel="stylesheet" />
    <link rel="stylesheet" href="mizzou.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Political Information, New Media, and Twitter bots
### <p>Michael W. KearneyðŸ“Š<br> School of Journalism<br> Informatics Institute<br> University of Missouri</p>
### <table style="border-style:none; padding: 0 0 50px 0; background-color:transparent" class="table">
<tr>
<th style="padding:0 30px">
<a href="https://twitter.com/kearneymw"><i class="fa fa-twitter"></i></a>
</th>
<th style="padding:0 30px">
<a href="https://github.com/mkearney"> <i class="fa fa-github"> </i></a>
</th>
</tr>
<tr style="background-color:transparent">
<th style="padding:0 30px">
<a href="https://twitter.com/kearneymw"> <span class="citation">@kearneymw</span> </a>
</th>
<th style="padding:0 30px">
<a href="https://github.com/mkearney"> <span class="citation">@mkearney</span> </a>
</th>
</tr>
</table>

---




class: center, middle

Presentation created using **{xaringan}** (the **robot** theme)

Slides available at [https://mkearney.github.io/dsa_execweek_talk](https://mkearney.github.io/dsa_execweek_talk)

---

background-size: 150px auto
background-position: 490px 185px, 567px 320px, 644px 185px, 721px 320px, 644px 455px, 567px 50px
background-image: url(img/chr-logo.png), url(img/hexagon-logo.png), url(img/textfeatures-logo.png), url(img/tfse-logo.png), url(img/botrnot-logo.png), url(img/rtweet-logo.svg)

# About Me

**Background**

- PhD in COMS from Kansas

**Research interests**

- Partisan selective exposure
- Digital and social media

**\#rstats packages**

- Twitter APIs: [**{rtweet}**](https://cran.r-project.org/package=rtweet)
- Text analysis: [**{textfeatures}**](https://cran.r-project.org/package=textfeatures) [**{chr}**](https://github.com/mkearney/chr/)
- Data wrangling/viz: [**{hexagon}**](https://github.com/mkearney/hexagon/) [**{tfse}**](https://github.com/mkearney/tfse/)
- Machine learning: [**{botrnot}**](https://github.com/mkearney/botrnot/)

**Shiny apps**

- Web app interface for [botrnot](https://mikewk.shinyapps.io/botornot/)
- [Interactive friendship tool](https://mikewk.shinyapps.io/friendship/)

---

# Outline

+ Public concern about automated and manipulative "bot" accounts
   - "Fake news" and bots during the 2016 election
   - Bots create traffic/amplify social divisions

+ Detection of bots on Twitter
   - Can we classify bots in real time to help people identify artificial accounts/trends?
   - How do we build a train/test data set?

+ Future directions
   - This all begs the question, what is a "Twitter bot"?

---

# 2016 election

Concerns about bots on social media manipulating public opinion spiked during the 2016 general election. 

The most alarming concerns were all related to behaviors of Kremlin-linked bot accounts for:

+ pushing fake news stories

+ amplifying social divisions

+ intesifying partisan disagreements

---

# Research questions

How do we **identify bots on social media**?

How can we do it in **real-time** so as to filter out inauthentic social media traffic?

---

# Social media data

**Reactions** to recent news about Cambridge Analytica

1. **Facebook was negligent** because it inadequately regulated third party apps

1. **Cambridge Analytica was unethical** because it stole and used data they shouldn't have

&amp;nbsp;

In other words...people don't know exactly what FB and CA actually did, but they know it was wrong.

---


# Social media APIs

Social media platforms encourage and regulate third party applications via **APIs**
+ **Application Program Interfaces** (APIs) are sets of protocols that govern interactions between sites and users

Similar to web browsers but with different primary objective:
+ Web browsers **render content**
+ Web APIs **organize data**

For public APIs, many sites only allow access to **authorized** users
+ Twitter, Facebook, Instagram, Github, etc.


---

# Twitter vs. Facebook

For a case study about political bots on social media, Twitter is

1. More open about its **data**
   + Unlike Facebook, which actually clamped down on data sharing (with developers) starting in 2014, **Twitter is quite open with its data**

1. More **public facing**
   + **Default privacy setting** is public
   + User **connections** are asymmetrical

---

# Twitter APIs

To connect to Twitter's APIs, users need a **consumer key** and **consumer secret**, which you can get by creating a Twitter app.

View the built-in vignette **`auth`** for instructions on obtaining and storing user access tokens.&lt;sup&gt;\* &lt;/sup&gt;


```r
## view rtweet's authorization vignette
vignette("auth", package = "rtweet")
```

Or look up the vignette online: [rtweet.info/articles/auth.html](http://rtweet.info/articles/auth.html)

.footnote[
&lt;sup&gt; \* &lt;/sup&gt; For a while, rtweet users were not required to create their own Twitter applications. [That recently changed](https://github.com/mkearney/rtweet/issues/179#issuecomment-365689068), but hopefully it's resolved soon.
]

---
class: inverse, center, middle


# Obtaining and using Twitter API keys

---


# Create a Twitter App

&lt;span&gt;1.&lt;/span&gt; Log in to your [Twitter.com](https://twitter.com) account

&lt;span&gt;2.&lt;/span&gt; Go to [apps.twitter.com](https://apps.twitter.com/) and select **Create New App**

&lt;span style="margin-left:15%"&gt;&lt;img src="img/createnewapp.png" width="60%" height="auto" /&gt;&lt;/span&gt;

---

&lt;span&gt;3.&lt;/span&gt; Complete the fields (**important**: Callback URL must be exact)
  - Name: `{{initials}}_twitter_app`
  - Description: `{{something about analyzing Twitter data}}`
  - Website: `https://twitter.com/{{you_screen_name}}`
  - **Callback URL:** `http://127.0.0.1:1410`

&lt;span style="margin-left:15%"&gt;&lt;img src="img/creating.png" width="60%" height="auto" /&gt;&lt;/span&gt;

&lt;span&gt;4.&lt;/span&gt; Store name of app as `app_name` in R script


```r
## name of twitter app
app_name &lt;- "mwk_twitter_app"
```

---


# Copy API keys

&lt;span&gt;5.&lt;/span&gt; Click **Create your Twitter application**

&lt;span&gt;6.&lt;/span&gt; Select **Keys and Access Tokens** tab

&lt;span&gt;7.&lt;/span&gt; Copy the **Consumer Key (API Key)** and **Consumer Secret (API Secret)** and paste into R script

&lt;span style="margin-left:15%"&gt;&lt;img src="img/keys.png" width="60%" height="auto" /&gt;&lt;/span&gt;


```r
## copy and pasted *your* keys (these are fake)
consumer_key &lt;- "XYznzPFOFZR2a39FwWKN1Jp41"
consumer_secret &lt;- "CtkGEWmSevZqJuKl6HHrBxbCybxI1xGLqrD5ynPd9jG0SoHZbD"
```

---

# Create your token

&lt;span&gt;8.&lt;/span&gt; Pass the stored values to `create_token()` to create the Twitter authorization token.


```r
## create token
token &lt;- create_token(app_name, consumer_key, consumer_secret)

## print token
token
```


---

# Storing your token

Every request sent to Twitter must include a token. Save yourself trouble by storing it as an environment variable.


```r
## save token to home directory
path_to_token &lt;- file.path(path.expand("~"), ".twitter_token.rds")
saveRDS(token, path_to_token)

## create env variable TWITTER_PAT (with path to saved token)
env_var &lt;- paste0("TWITTER_PAT=", path_to_token)

## save as .Renviron file (or append if the file already exists)
cat(env_var, file = file.path(path.expand("~"), ".Renviron"),
  fill = TRUE, append = TRUE)
```

Normally `.Renviron` is processed on start-up, so refresh it.


```r
## refresh .Renviron variables
readRenviron("~/.Renviron")
```


---
class: inverse, center, middle

# Twitter data

---

# Getting the data

Functions in **{rtweet}** correspond with different [API] paths. For example, use `rtweet::get_timeline()` to return up to the most recent 3,2000 tweets posted by a given user.


```r
cnn &lt;- get_timeline("cnn", n = 3200)
ts_plot(cnn, "hours")
```

&lt;span style="margin-left:1%"&gt;&lt;img src="img/cnn.png" width="90%" height="auto" /&gt;&lt;/span&gt;

---
class: inverse, center, middle

# Identifying bots

---

# Current approaches

The creators of [**botometer**]() maintain a list open sourced academic studies that identified bots.

But this approach is not without **limitations**:
+ Academic research moves slowly (especially with Twitter banning many of the bots)
+ Relatively small number have been collected
+ Labour intensive methods (human coding)

---

# New approach

One **solution** to these limitations is to combine the academic sources with an easy-to-automate method that takes advantage of naturally occcurring human coding

1. Select a handful of previously verified bots
1. Look up the public **lists** that include those bots
1. Identify lists **names** that self identify as bot lists

---
class: inverse, center, middle

# Compiling data sets

---

# User/tweet data

Using the list of screen names for non-bot (based on previous academic research and human coding) and bot accounts, I gathered...

+ **tweet-level data** for the most recent 100 tweets posted by each user `rtweet::get_timelines()`

+ **user-level (or account-level) data** for each user `rtweet::lookup_users()`

The data returned by Twitter consists of over 60 features related to the tweet and author (user) of each tweet.

---

# Data sets

Randomly sample Twitter data into train and test data sets

+ **Train data**: approx. 70% of users in sample
   - Making sure there are a roughly equal number of bots and non-bots

+ **Test data**: remaining (30%) bot and non-bot accounts

---

# Features

In addition to the numeric variables already returned by Twitter, I created the **{textfeatures}** package, which extracts 26 features from a supplied character vector.

The text features were extracted for:

+ Tweets
+ Name
+ Screen name
+ Location
+ Description (bio)

---
class: inverse, center, middle

# Modeling

---

# Two models

Two models were created. 

1. The **default model**, which uses both users and tweets data.

1. The **fast model**, which uses only users data

---

# Model results

A gradient boosted logistic model **{gbm::gbm()}** was trained and tested on the two data sets for both the default and fast models.

+ The **default model** was 93.53% accurate when classifying bots and 95.32% accurate when classifying non-bots

+ The **fast model** was 91.78% accurate when classifying bots and 92.61% accurate when classifying non-bots

Overall...

+ The **default model** was correct 93.8% of the time

+ The **fast model** was correct 91.9% of the time.

---

# Applications

+ This has since been assembled into an R package [**{botrnot}**](https://github.com/mkearney/botrnot)---coming soon to CRAN!

+ It has also been exported as a [Shiny web app](https://mikewk.shinyapps.io/botornot)

---
class: inverse, center, middle

# The end
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function() {
  var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})();</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
  }
});
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
